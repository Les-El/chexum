# Hashi Feature Integration Guide
*Decisions from checkpoint review 260116-2106*

## New Features to Implement

### 1. Archive Metadata Validation
**Flags:** `--meta` and `--checksum` (aliases)
**Behavior:** 
- `hashi file.zip --meta` validates embedded CRC32 checksums
- Always display security warning about CRC32 vs cryptographic hashes
- Works with boolean output: `hashi file.zip --meta -b` but still returns warning
- warning only silenced with a manual variable change
**Security Warning System:**
- Environment-based control (first-run warnings with altered exit codes)
- Requires intentional discovery of environment variables
- Temporary/local/global scope options

### 2. Bulk Verification Flags
**New flags:**
- `--filelist <file>` - read file paths from file
- `--hashlist <file>` - read hash strings from file  
- `--mixedlist <file>` - read mixed file paths and hashes from file
**Purpose:** Handle separate lists of hashes and files without manual loops

### 3. Early Exit Controls
**New flags:**
- `--fail-fast <N>` - exit after N mismatches
- `--fail-limit <N>` - allow up to N mismatches before aborting
    Q: Is there any functional difference that requires two flags?
**Use case:** CI/CD pipelines where early failure detection saves time

### 4. Result Filtering
**New flags:**
- `--return-matches` - only show successful matches
- `--return-mismatches` - only show failures/mismatches
- Can be used together; output order follows flag order
**Use case:** Finding specific results in large datasets

### 5. External Configuration
**New flag:** `--flaglist <file>` - load flags from text file
**Complements:** `@file` syntax for reading arguments from file
**Purpose:** Overcome command line length limits (ARG_MAX)

### 6. Explicit Argument Typing
**New flags:**
- `--hasharg <string>` - force treatment as hash string
- `--filearg <path>` - force treatment as file path
**Purpose:** Disambiguation when 64-char hex filenames could be confused with hashes

## Core Features to Enhance

### Automatic Algorithm Detection
- Hash string length automatically maps to algorithm
- Some hash alogos return the same length strings - secondary operation to resolve collision
- Core feature: `hashi <hash_string>` validates or fails
- No flags required for basic validation

### Error Messaging
**Pattern:** `[Error Category] + [What Happened] + [Suggested Action]`
- Actionable errors by default
- Detailed explanations only in `--verbose` output
- Keep messages close to code (no centralized message system)
- Use Go's error wrapping: `fmt.Errorf("reading %s: %w", file, err)`

### File Output Security
**Security measures:**
- File extension validation (`.txt`, `.json`, `.csv` only)
- Path traversal prevention (block `..` sequences)
- Permission validation before writing
- Refuse to append to executable files
- Basic format validation for target file type

## Architecture Guidelines

### Pragmatic Atomic Approach
**When to share logic:**
- Operations that are logically identical (not just similar)
- Hash comparison across multiple output formats
- File reading operations used by multiple features
- Progress bar systems needed by different flags

**When NOT to share:**
- Different operations (archive validation vs file hashing)
- Different error contexts (network vs local files)
- Similar but not identical operations

### Architecture Enhancement
**Current 5-layer architecture enhanced with pipeline concepts:**
- **Configuration Layer**: Add early filtering and argument classification
- **Processing Layer**: Maintain linear data flow through operations
- **Output Layer**: Keep existing formatter flexibility
- **Control Layer**: Preserve signal handling as-is

**Pipeline concepts to adopt:**
- Early filtering: Apply size/date/pattern filters before expensive operations
- Argument classification: Separate files vs hashes in Configuration Layer
- Linear flow: Ensure predictable data progression through layers
- Maintain flexibility: Don't enforce rigid pipeline constraints

**Rejected approach:** Full 7-layer pipeline (over-engineering for CLI tool scope)

### Feature Evaluation Criteria
**Accept features that:**
1. Provide clear user benefit
2. Fill real workflow gaps that multiply productivity
3. Address high-frequency workflows or common pain points
4. Solve error-prone manual processes

**Reject features that:**
- Are niche use cases solvable with simple shell scripts
- Add complexity without sufficient benefit
- Can be achieved by combining existing functionality

## Rejected Proposals

### Not to Implement
- **Duplicate thresholds** (`--min-matches N`) - niche use case
- **Directory set comparison** (`--diff dirA dirB`) - solvable with existing tools
- **Manifest-as-input** - users can script this with pipes
- **Centralized message system** - creates more problems than it solves

## Integration Priority

### High Priority (Core Features)
1. Automatic algorithm detection
2. Archive metadata validation with security warnings
3. Explicit argument typing (`--hasharg`/`--filearg`)

### Medium Priority (Workflow Enhancements)
1. Bulk verification flags (`--filelist`, `--hashlist`, `--mixedlist`)
2. Early exit controls (`--fail-fast`, `--fail-limit`)
3. Result filtering (`--return-matches`, `--return-mismatches`)

### Low Priority (Convenience Features)
1. External flag files (`--flaglist`)
2. File output security enhancements

## Implementation Notes

- All new flags must integrate with existing conflict resolution system
- Boolean output (`-b`) must work consistently across all new features
- Security warnings use environment variables, not force flags
- Sequential output ordering follows flag order on command line
    *EVALUATE THIS POINT*: The vision of hashi includes easy of use and versitility. The order of flags and arguments given should not matter *execpt in cases where we have chosen to do so to resolve conflicts or collisions.* This will effect the way we build the input parsing and logic.
- Maintain backward compatibility with existing functionality

## Core Use Cases Confirmed
All identified use cases align with hashi vision:
- De-duplication: Finding files with identical content
- Download verification: Ensuring downloaded files match provided checksums
- Bulk auditing: Creating manifests and checking for changes over time
- Archive integrity: Checking archive metadata checksums
- CI/CD automation: Integration with pipelines and scripts